#!/usr/bin/env python3

import matplotlib.pyplot as plt
from datetime import datetime
import sys
import json
import urllib.request
import urllib.error
import os
import pickle
from pathlib import Path

def get_cache_file():
    """Get the cache file path using XDG standards."""
    if os.name == 'posix':
        cache_dir = Path.home() / '.cache' / 'mise-track'
    else:  # Windows
        cache_dir = Path.home() / 'AppData' / 'Local' / 'mise-track'

    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir / 'data.pkl'

CACHE_FILE = get_cache_file()

def load_cache():
    """Load cached data if it exists."""
    if CACHE_FILE.exists():
        try:
            with open(CACHE_FILE, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            print(f"Error loading cache: {e}")
    return {'commits': [], 'file_counts': {}}

def save_cache(cache):
    """Save data to cache."""
    try:
        with open(CACHE_FILE, 'wb') as f:
            pickle.dump(cache, f)
    except Exception as e:
        print(f"Error saving cache: {e}")

def check_rate_limit():
    """Check GitHub API rate limit."""
    url = "https://api.github.com/rate_limit"
    try:
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'mise-track-script')
        with urllib.request.urlopen(req) as response:
            data = json.loads(response.read())
            return data['rate']['remaining'], datetime.fromtimestamp(data['rate']['reset'])
    except:
        return None, None

def fetch_commits():
    """Fetch all commits from GitHub."""
    print("Fetching commit history...")
    all_commits = []
    page = 1

    while True:
        url = f"https://api.github.com/repos/chrishwiggins/mise/commits?page={page}&per_page=100"
        try:
            req = urllib.request.Request(url)
            req.add_header('User-Agent', 'mise-track-script')
            with urllib.request.urlopen(req) as response:
                commits = json.loads(response.read())
                if not commits:
                    break
                all_commits.extend(commits)
                print(f"  Fetched {len(all_commits)} commits...")
                if len(commits) < 100:
                    break
                page += 1
        except urllib.error.HTTPError as e:
            if e.code == 403:
                print(f"Rate limited while fetching commits (got {len(all_commits)} so far)")
                break
            raise

    # Reverse to chronological order and sample
    all_commits.reverse()

    if len(all_commits) > 100:
        step = len(all_commits) // 100
        sampled = [all_commits[i] for i in range(0, len(all_commits), step)]
        if all_commits[-1] not in sampled:
            sampled.append(all_commits[-1])
        return sampled
    return all_commits

def fetch_file_count(sha):
    """Get file count for a specific commit."""
    url = f"https://api.github.com/repos/chrishwiggins/mise/git/trees/{sha}?recursive=1"
    try:
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'mise-track-script')
        with urllib.request.urlopen(req) as response:
            data = json.loads(response.read())
            return len([item for item in data.get('tree', []) if item['type'] == 'blob'])
    except urllib.error.HTTPError as e:
        if e.code == 403:
            return None  # Rate limited
        return 0
    except:
        return 0

def main():
    if '--check' in sys.argv:
        remaining, reset_dt = check_rate_limit()
        if remaining is not None:
            print(f"API calls remaining: {remaining}/60")
            print(f"Resets at: {reset_dt.strftime('%H:%M:%S')}")
        sys.exit(0)

    if '--fresh' in sys.argv:
        if CACHE_FILE.exists():
            CACHE_FILE.unlink()
            print(f"Cache cleared ({CACHE_FILE})")

    # Load existing cache
    cache = load_cache()

    # Step 1: Get commits if we don't have them
    if not cache.get('commits'):
        remaining, _ = check_rate_limit()
        if remaining and remaining < 10:
            print(f"Only {remaining} API calls left - not enough to fetch commits")
            print("Wait for rate limit reset or use --cached")
            sys.exit(1)

        cache['commits'] = fetch_commits()
        if not cache['commits']:
            print("Failed to fetch commits")
            sys.exit(1)
        save_cache(cache)
        print(f"Cached {len(cache['commits'])} commits")

    commits = cache['commits']
    file_counts = cache.get('file_counts', {})

    # Step 2: Identify missing data
    missing_shas = [c['sha'] for c in commits if c['sha'] not in file_counts]

    print(f"Analysis: {len(commits)} commits total")
    print(f"Have data for: {len(file_counts)} commits")
    print(f"Missing data: {len(missing_shas)} commits")

    # Step 3: Fetch missing data if we have API calls
    if missing_shas:
        remaining, reset_dt = check_rate_limit()

        if remaining is None:
            print("Cannot check rate limit - proceeding cautiously")
            fetch_limit = 10
        elif remaining < 5:
            print(f"Only {remaining} API calls left - need to wait for rate limit reset")
            print(f"Rate limit resets at {reset_dt.strftime('%H:%M:%S')}")

            # Calculate wait time
            import time
            wait_time = (reset_dt - datetime.now()).total_seconds() + 1
            if wait_time > 0:
                print(f"Sleeping {wait_time:.0f} seconds ({wait_time/60:.1f} minutes) for rate limit to reset...")
                time.sleep(wait_time)
                # Re-check rate limit after waiting
                remaining, reset_dt = check_rate_limit()
                if remaining and remaining > 5:
                    fetch_limit = max(0, remaining - 3)
                    print(f"Rate limit reset! Will fetch {min(fetch_limit, len(missing_shas))} missing data points")
                else:
                    fetch_limit = 0
            else:
                fetch_limit = 0
        else:
            # Use most of remaining calls, leave a few as buffer
            fetch_limit = max(0, remaining - 3)
            print(f"Will attempt to fetch {min(fetch_limit, len(missing_shas))} missing data points")

        fetched = 0
        for sha in missing_shas[:fetch_limit]:
            count = fetch_file_count(sha)
            if count is None:  # Rate limited
                print(f"Hit rate limit after fetching {fetched} commits")
                break
            file_counts[sha] = count
            fetched += 1

            if fetched % 10 == 0:
                print(f"  Fetched {fetched}/{min(fetch_limit, len(missing_shas))}")

        if fetched > 0:
            cache['file_counts'] = file_counts
            save_cache(cache)
            print(f"Fetched {fetched} new data points")

    # Step 4: Generate plot with available data
    dates, counts = [], []
    for commit in commits:
        sha = commit['sha']
        if sha in file_counts:
            date = datetime.fromisoformat(commit['commit']['committer']['date'].replace('Z', '+00:00'))
            dates.append(date)
            counts.append(file_counts[sha])

    if not dates:
        print("No data available to plot")
        sys.exit(1)

    # Create plot
    plt.figure(figsize=(12, 6))
    plt.plot(dates, counts, marker='o', markersize=3, linewidth=1.5)
    plt.title('mise Repository File Count Over Time', fontsize=14, fontweight='bold')
    plt.xlabel('Date')
    plt.ylabel('Number of Files')
    plt.grid(True, alpha=0.3)
    plt.gcf().autofmt_xdate()

    # Add completion info
    completion = len(file_counts) / len(commits) * 100
    plt.text(0.02, 0.98, f'Current files: {counts[-1]}\nData: {completion:.0f}% complete',
            transform=plt.gca().transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    plt.savefig('/tmp/mise-track.png', dpi=150, bbox_inches='tight')
    print(f"\nPlot saved to /tmp/mise-track.png")

    print(f"\nStats: {len(dates)} data points, {len(missing_shas)} still missing")
    if missing_shas:
        print("Run again to fetch more data")

if __name__ == "__main__":
    main()