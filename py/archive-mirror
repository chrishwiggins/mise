#!/usr/bin/env python3
"""
Archive a URL to the Wayback Machine.
Checks if the URL has been archived, and saves it if not.
Supports recursive archiving with -down flag.
"""

import argparse
import requests
import sys
import time
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup


def check_archived(url, verbose=True):
    """Check if a URL has been archived in the Wayback Machine."""
    api_url = f"https://archive.org/wayback/available?url={url}"

    try:
        response = requests.get(api_url)
        response.raise_for_status()
        data = response.json()

        if data.get("archived_snapshots") and data["archived_snapshots"].get("closest"):
            snapshot = data["archived_snapshots"]["closest"]
            timestamp = snapshot.get("timestamp")
            archive_url = snapshot.get("url")

            # Parse timestamp (format: YYYYMMDDhhmmss)
            if timestamp:
                dt = datetime.strptime(timestamp, "%Y%m%d%H%M%S")
                if verbose:
                    print(f"✓ Already archived on {dt.strftime('%Y-%m-%d %H:%M:%S')}")
                    print(f"  Archive URL: {archive_url}")
                return True

        if verbose:
            print("✗ Not archived yet")
        return False

    except Exception as e:
        if verbose:
            print(f"Error checking archive status: {e}")
        return False


def save_to_archive(url, verbose=True):
    """Save a URL to the Wayback Machine."""
    save_url = f"https://web.archive.org/save/{url}"

    if verbose:
        print(f"Saving {url} to Wayback Machine...")

    try:
        response = requests.post(save_url, timeout=120)
        response.raise_for_status()

        # The response headers contain the job ID and other info
        job_id = response.headers.get("job_id")

        if response.status_code == 200:
            if verbose:
                print(f"✓ Successfully saved to archive!")
                if job_id:
                    print(f"  Job ID: {job_id}")
            return True
        else:
            if verbose:
                print(f"Unexpected response: {response.status_code}")
            return False

    except Exception as e:
        if verbose:
            print(f"Error saving to archive: {e}")
        return False


def get_links_from_url(url):
    """Fetch a URL and extract all links from it."""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        links = set()
        for link in soup.find_all("a", href=True):
            href = link["href"]
            # Convert relative URLs to absolute
            absolute_url = urljoin(url, href)
            links.add(absolute_url)

        return links

    except Exception as e:
        print(f"Error fetching links from {url}: {e}")
        return set()


def same_domain(url1, url2):
    """Check if two URLs are from the same domain."""
    parsed1 = urlparse(url1)
    parsed2 = urlparse(url2)
    return parsed1.netloc == parsed2.netloc


def get_parent_url(url):
    """Get the parent directory URL."""
    parsed = urlparse(url)
    path = parsed.path

    # Remove trailing slash if present
    if path.endswith("/"):
        path = path[:-1]

    # Go up one directory
    if "/" in path:
        parent_path = path.rsplit("/", 1)[0]
        if not parent_path:
            parent_path = "/"
    else:
        parent_path = "/"

    # Ensure trailing slash for directory URLs
    if parent_path != "/" and not parent_path.endswith("/"):
        parent_path += "/"

    return f"{parsed.scheme}://{parsed.netloc}{parent_path}"


def archive_recursive(url, max_depth=2, current_depth=0, visited=None, force=False):
    """
    Recursively archive a URL and all links on the same domain.

    Args:
        url: The URL to archive
        max_depth: Maximum recursion depth
        current_depth: Current recursion depth
        visited: Set of already visited URLs
        force: Force archiving even if already archived
    """
    if visited is None:
        visited = set()

    # Normalize URL (remove fragments)
    parsed = urlparse(url)
    normalized_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
    if parsed.query:
        normalized_url += f"?{parsed.query}"

    # Skip if already visited
    if normalized_url in visited:
        return visited

    visited.add(normalized_url)

    # Print depth indicator
    indent = "  " * current_depth
    print(f"{indent}[Depth {current_depth}] {normalized_url}")

    # Check and archive this URL
    is_archived = check_archived(normalized_url, verbose=False)
    if force or not is_archived:
        if force and is_archived:
            print(f"{indent}  → Already archived, but forcing new snapshot...")
        else:
            print(f"{indent}  → Archiving...")
        save_to_archive(normalized_url, verbose=False)
        # Rate limiting - be nice to archive.org
        time.sleep(2)
    else:
        print(f"{indent}  → Already archived")

    # If we've reached max depth, don't recurse further
    if current_depth >= max_depth:
        return visited

    # Get links from this page
    print(f"{indent}  → Finding links...")
    links = get_links_from_url(normalized_url)

    # Filter to same domain and not yet visited
    same_domain_links = [
        link for link in links if same_domain(url, link) and link not in visited
    ]

    print(f"{indent}  → Found {len(same_domain_links)} new links on same domain")

    # Recursively archive each link
    for link in same_domain_links:
        archive_recursive(link, max_depth, current_depth + 1, visited, force)
        # Rate limiting between pages
        time.sleep(1)

    return visited


def main():
    parser = argparse.ArgumentParser(description="Archive URLs to the Wayback Machine")
    parser.add_argument("url", help="URL to archive")
    parser.add_argument(
        "-down",
        "--recursive",
        action="store_true",
        help="Recursively archive all links on the same domain",
    )
    parser.add_argument(
        "-up",
        "--parent",
        action="store_true",
        help="Do -down first, then go up one level and do -down there",
    )
    parser.add_argument(
        "-d",
        "--depth",
        type=int,
        default=2,
        help="Maximum recursion depth (default: 2)",
    )
    parser.add_argument(
        "-f",
        "--force",
        action="store_true",
        help="Force archiving even if already archived (create new snapshot)",
    )

    args = parser.parse_args()

    if args.parent:
        print(f"Starting parent-recursive archive of {args.url}")
        print(f"Max depth: {args.depth}")
        if args.force:
            print(f"Force mode: Creating new snapshots for all URLs")
        print(f"\nStep 1: Archiving current URL and children...")
        visited = archive_recursive(args.url, max_depth=args.depth, force=args.force)

        parent_url = get_parent_url(args.url)
        print(f"\nStep 2: Going up to parent: {parent_url}")
        print(f"Archiving parent and children...")
        visited_parent = archive_recursive(
            parent_url, max_depth=args.depth, force=args.force, visited=visited
        )

        print(f"\nComplete! Archived {len(visited_parent)} URLs total")
    elif args.recursive:
        print(f"Starting recursive archive of {args.url}")
        print(f"Max depth: {args.depth}")
        if args.force:
            print(f"Force mode: Creating new snapshots for all URLs")
        print(f"This may take a while...\n")
        visited = archive_recursive(args.url, max_depth=args.depth, force=args.force)
        print(f"\nComplete! Archived {len(visited)} URLs")
    else:
        print(f"Checking archive status for: {args.url}\n")
        is_archived = check_archived(args.url)

        if args.force or not is_archived:
            if args.force and is_archived:
                print("\nAlready archived, but forcing new snapshot...")
            else:
                print("\nAttempting to save to archive...")
            save_to_archive(args.url)
        else:
            print(
                "\nPage is already archived. To create a new snapshot, use the -f flag."
            )


if __name__ == "__main__":
    main()
